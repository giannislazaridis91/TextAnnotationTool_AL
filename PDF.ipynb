{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Images from the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for working with PDF files.\n",
    "from PIL import Image  # Python Imaging Library for handling images.\n",
    "import io  # Core tools for working with I/O operations.\n",
    "\n",
    "def is_valid_image(image_bytes, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Check if the image is valid (not mostly black).\n",
    "    \"\"\"\n",
    "    with Image.open(io.BytesIO(image_bytes)) as img:  # Open the image from bytes.\n",
    "        img = img.convert(\"RGB\")  # Convert the image to RGB.\n",
    "        pixels = img.load()  # Load the pixel data.\n",
    "        width, height = img.size  # Get the dimensions of the image.\n",
    "        black_pixel_count = 0  # Initialize the count for black pixels.\n",
    "        total_pixels = width * height  # Calculate the total number of pixels.\n",
    "        \n",
    "        # Count black pixels.\n",
    "        for x in range(width):  # Loop over the width of the image.\n",
    "            for y in range(height):  # Loop over the height of the image.\n",
    "                r, g, b = pixels[x, y]  # Get the RGB values of the pixel.\n",
    "                if r == 0 and g == 0 and b == 0:  # Check if the pixel is black.\n",
    "                    black_pixel_count += 1  # Increment the black pixel count.\n",
    "\n",
    "        # Calculate the percentage of black pixels.\n",
    "        black_pixel_percentage = black_pixel_count / total_pixels  # Compute the percentage.\n",
    "        return black_pixel_percentage < threshold  # Return if the black pixel percentage is below the threshold.\n",
    "\n",
    "def save_images_from_pdf(pdf_path, output_folder):\n",
    "    # Open the PDF file.\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF.\n",
    "    \n",
    "    # Iterate through each page in the PDF.\n",
    "    for page_num in range(len(pdf_document)):  # Loop over each page in the PDF.\n",
    "        page = pdf_document.load_page(page_num)  # Load the specific page.\n",
    "        image_list = page.get_images(full=True)  # Get the list of images on the page.\n",
    "        \n",
    "        for img_index, img in enumerate(image_list):  # Loop over each image in the list.\n",
    "            xref = img[0]  # Get the reference number for the image.\n",
    "            base_image = pdf_document.extract_image(xref)  # Extract the image from the PDF.\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes.\n",
    "            \n",
    "            if is_valid_image(image_bytes):  # Check if the image is valid.\n",
    "                image_filename = f\"{output_folder}/page_{page_num + 1}_img_{img_index + 1}.png\"  # Define the filename for the image.\n",
    "                \n",
    "                with open(image_filename, \"wb\") as img_file:  # Open the file in write-binary mode.\n",
    "                    img_file.write(image_bytes)  # Write the image bytes to the file.\n",
    "                print(f\"Saved image {image_filename}\")  # Print a message indicating the image was saved.\n",
    "            else:\n",
    "                print(f\"Skipped invalid image from page {page_num + 1}, image {img_index + 1}.\")  # Print a message indicating the image was skipped.\n",
    "    \n",
    "    print(\"Finished extracting images.\")  # Print a message indicating the process is finished.\n",
    "    pdf_document.close()  # Close the PDF document.\n",
    "\n",
    "# Example usage.\n",
    "pdf_path = \"Animal_Pictures.pdf\"  # Path for the PDF file defined here.\n",
    "output_folder = \"Output_Images\"  # Define the output folder for saving images.\n",
    "\n",
    "# Ensure the output folder exists.\n",
    "import os  # Import the OS module for operating system dependent functionality.\n",
    "if not os.path.exists(output_folder):  # Check if the output folder does not exist.\n",
    "    os.makedirs(output_folder)  # Create the output folder if it does not exist.\n",
    "\n",
    "save_images_from_pdf(pdf_path, output_folder)  # Call the function to save images from the PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a .json file as an example.\n",
    "\n",
    "import json\n",
    "\n",
    "annotations = {\n",
    "    \"annotations\": [\n",
    "        {\"type\": \"image\", \"page\": 1, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"cow\"},\n",
    "        {\"type\": \"image\", \"page\": 1, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"cow\"},\n",
    "        {\"type\": \"image\", \"page\": 3, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"goat\"},\n",
    "        {\"type\": \"image\", \"page\": 3, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"goat\"},\n",
    "        {\"type\": \"image\", \"page\": 5, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"lama\"},\n",
    "        {\"type\": \"image\", \"page\": 5, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"sheep\"},\n",
    "        {\"type\": \"image\", \"page\": 7, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"chicken\"},\n",
    "        {\"type\": \"image\", \"page\": 7, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"chicken\"},\n",
    "        {\"type\": \"image\", \"page\": 9, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"horse\"},\n",
    "        {\"type\": \"image\", \"page\": 9, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"horse\"},\n",
    "        {\"type\": \"image\", \"page\": 11, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"pig\"},\n",
    "        {\"type\": \"image\", \"page\": 11, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"pig\"},\n",
    "        {\"type\": \"image\", \"page\": 13, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"cow\"},\n",
    "        {\"type\": \"image\", \"page\": 13, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"chicken\"},\n",
    "        {\"type\": \"image\", \"page\": 15, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"sheep\"},\n",
    "        {\"type\": \"image\", \"page\": 15, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"horse\"},\n",
    "        {\"type\": \"image\", \"page\": 17, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"pig\"},\n",
    "        {\"type\": \"image\", \"page\": 17, \"coordinates\": {\"x\": 0, \"y\": 0, \"width\": 300, \"height\": 200}, \"label\": \"sheep\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('./annotations.json', 'w') as json_file:\n",
    "    json.dump(annotations, json_file, indent=4)\n",
    "\n",
    "print(\"New annotations JSON file has been created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open('./annotations.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = './Output_Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image  # Import the Image class from the PIL (Pillow) library to handle image operations.\n",
    "import numpy as np  # Import NumPy for array manipulations.\n",
    "\n",
    "def load_images_and_labels(output_folder, annotations):\n",
    "    length_of_annotations = len(annotations['annotations'])  # Determine the number of annotations provided.\n",
    "    images = []  # Initialize an empty list to store image arrays.\n",
    "    labels = []  # Initialize an empty list to store corresponding labels.\n",
    "    image_paths = []  # Initialize an empty list to keep track of image paths processed.\n",
    "\n",
    "    for annotation in annotations['annotations']:  # Iterate over each annotation in the annotations list.\n",
    "        page = annotation['page']  # Extract the page number from the annotation.\n",
    "        label = annotation['label']  # Extract the label from the annotation.\n",
    "        x, y, width, height = annotation['coordinates'].values()  # Extract the bounding box coordinates from the annotation.\n",
    "\n",
    "        for i in range(length_of_annotations):  # Loop through all possible image files based on the length of annotations.\n",
    "            image_path = f\"{output_folder}/page_{page}_img_{i}.png\"  # Construct the path to the image file.\n",
    "            if os.path.exists(image_path):  # Check if the image file exists at the specified path.\n",
    "                if image_path not in image_paths:  # Ensure the image path has not been processed yet.\n",
    "                    print(\"image_path\", image_path)  # Print the image path for debugging or logging purposes.\n",
    "                    with Image.open(image_path) as img:  # Open the image file.\n",
    "                        img = img.convert('RGB')  # Convert the image to RGB mode to ensure it's in a standard format.\n",
    "                        cropped_img = img.crop((x, y, x + width, y + height))  # Crop the image using the coordinates provided in the annotation.\n",
    "                        img_array = np.array(cropped_img)  # Convert the cropped image to a NumPy array.\n",
    "                        images.append(img_array)  # Append the image array to the images list.\n",
    "                        labels.append(label)  # Append the corresponding label to the labels list.\n",
    "                        image_paths.append(image_path)  # Add the image path to the list of processed paths.\n",
    "\n",
    "    return np.array(images), np.array(labels)  # Convert the lists to NumPy arrays and return them.\n",
    "\n",
    "images, labels = load_images_and_labels(output_folder, annotations)  # Call the function with the specified output folder and annotations.\n",
    "\n",
    "print(images.shape, labels)  # Print the shape of the images array and the labels list to verify the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Classifier Using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Import PyTorch library.\n",
    "import torch.nn as nn  # Import neural network module from PyTorch.\n",
    "import torch.optim as optim  # Import optimization module from PyTorch.\n",
    "from torch.utils.data import DataLoader, Dataset  # Import data handling utilities from PyTorch.\n",
    "from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split  # Import train_test_split from scikit-learn.\n",
    "import torchvision.transforms as transforms  # Import transforms from torchvision for image preprocessing.\n",
    "\n",
    "# Define the dataset class.\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # Assign images to the class variable.\n",
    "        self.labels = labels  # Assign labels to the class variable.\n",
    "        self.transform = transform  # Assign transform to the class variable.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)  # Return the total number of samples in the dataset.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Get the image at the specified index.\n",
    "        label = self.labels[idx]  # Get the label at the specified index.\n",
    "        \n",
    "        if self.transform:  # If transform is provided.\n",
    "            image = self.transform(image)  # Apply the transform to the image.\n",
    "        \n",
    "        return image, label  # Return the image and its label.\n",
    "\n",
    "# Preprocess images and labels.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors.\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64 pixels.\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize images to have mean 0.5 and std 0.5.\n",
    "])\n",
    "\n",
    "label_encoder = LabelEncoder()  # Initialize the LabelEncoder.\n",
    "labels_encoded = label_encoder.fit_transform(labels)  # Encode the string labels as integers.\n",
    "\n",
    "# Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)  # 80% training, 20% testing.\n",
    "\n",
    "train_dataset = AnimalDataset(X_train, y_train, transform=transform)  # Create the training dataset.\n",
    "test_dataset = AnimalDataset(X_test, y_test, transform=transform)  # Create the testing dataset.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create the training data loader.\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Create the testing data loader.\n",
    "\n",
    "# Define a simple CNN model.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()  # Initialize the parent class.\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # First convolutional layer.\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Second convolutional layer.\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # First fully connected layer.\n",
    "        self.fc2 = nn.Linear(128, len(label_encoder.classes_))  # Second fully connected layer.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Apply ReLU activation after the first convolutional layer.\n",
    "        x = torch.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2.\n",
    "        x = torch.relu(self.conv2(x))  # Apply ReLU activation after the second convolutional layer.\n",
    "        x = torch.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2.\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor.\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation after the first fully connected layer.\n",
    "        x = self.fc2(x)  # Apply the second fully connected layer.\n",
    "        return x  # Return the output.\n",
    "\n",
    "model = SimpleCNN()  # Instantiate the model.\n",
    "criterion = nn.CrossEntropyLoss()  # Define the loss function.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Define the optimizer.\n",
    "\n",
    "# Training loop.\n",
    "num_epochs = 1000  # Define the number of epochs for training.\n",
    "for epoch in range(num_epochs):  # Loop over the number of epochs.\n",
    "    model.train()  # Set the model to training mode.\n",
    "    running_loss = 0.0  # Initialize the running loss.\n",
    "    for images, labels in train_loader:  # Loop over the data in the dataloader.\n",
    "        optimizer.zero_grad()  # Zero the gradients.\n",
    "        outputs = model(images)  # Forward pass.\n",
    "        loss = criterion(outputs, labels)  # Compute the loss.\n",
    "        loss.backward()  # Backward pass.\n",
    "        optimizer.step()  # Update the parameters.\n",
    "        running_loss += loss.item()  # Accumulate the loss.\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(train_loader)}\")  # Print the average loss for the epoch.\n",
    "\n",
    "# Evaluate the model.\n",
    "model.eval()  # Set the model to evaluation mode.\n",
    "correct = 0  # Initialize the number of correct predictions.\n",
    "total = 0  # Initialize the total number of samples.\n",
    "with torch.no_grad():  # Disable gradient calculation.\n",
    "    for images, labels in test_loader:  # Loop over the data in the test loader.\n",
    "        outputs = model(images)  # Forward pass.\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class.\n",
    "        total += labels.size(0)  # Update the total number of samples.\n",
    "        correct += (predicted == labels).sum().item()  # Update the number of correct predictions.\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')  # Print the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using Uncertainty Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing PyTorch library.\n",
    "import torch.nn as nn  # Importing the neural network module from PyTorch.\n",
    "import torch.optim as optim  # Importing the optimization module from PyTorch.\n",
    "from torch.utils.data import DataLoader, Dataset, Subset  # Importing data handling utilities from PyTorch.\n",
    "from sklearn.preprocessing import LabelEncoder  # Importing LabelEncoder from scikit-learn.\n",
    "from sklearn.model_selection import train_test_split  # Importing train_test_split from scikit-learn.\n",
    "import torchvision.transforms as transforms  # Importing transforms from torchvision for image preprocessing.\n",
    "import numpy as np  # Importing NumPy for numerical operations.\n",
    "\n",
    "# Define the dataset class.\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images  # Assigning images to the class variable.\n",
    "        self.labels = labels  # Assigning labels to the class variable.\n",
    "        self.transform = transform  # Assigning transform to the class variable.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)  # Returning the total number of samples in the dataset.\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]  # Getting the image at the specified index.\n",
    "        label = self.labels[idx]  # Getting the label at the specified index.\n",
    "        \n",
    "        if self.transform:  # If transform is provided.\n",
    "            image = self.transform(image)  # Apply the transform to the image.\n",
    "        \n",
    "        return image, label  # Return the image and its label.\n",
    "\n",
    "# Preprocess images and labels.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors.\n",
    "    transforms.Resize((64, 64)),  # Resize images to 64x64 pixels.\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize images to have mean 0.5 and std 0.5.\n",
    "])\n",
    "\n",
    "label_encoder = LabelEncoder()  # Initialize the LabelEncoder.\n",
    "labels_encoded = label_encoder.fit_transform(labels)  # Encode the string labels as integers.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)  # Split the dataset into training and testing sets.\n",
    "\n",
    "train_dataset = AnimalDataset(X_train, y_train, transform=transform)  # Create the training dataset.\n",
    "test_dataset = AnimalDataset(X_test, y_test, transform=transform)  # Create the testing dataset.\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create the training data loader.\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Create the testing data loader.\n",
    "\n",
    "# Define a simple CNN model.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()  # Initialize the parent class.\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)  # First convolutional layer.\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # Second convolutional layer.\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)  # First fully connected layer.\n",
    "        self.fc2 = nn.Linear(128, len(label_encoder.classes_))  # Second fully connected layer.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))  # Apply ReLU activation after the first convolutional layer.\n",
    "        x = torch.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2.\n",
    "        x = torch.relu(self.conv2(x))  # Apply ReLU activation after the second convolutional layer.\n",
    "        x = torch.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2.\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor.\n",
    "        x = torch.relu(self.fc1(x))  # Apply ReLU activation after the first fully connected layer.\n",
    "        x = self.fc2(x)  # Apply the second fully connected layer.\n",
    "        return x  # Return the output.\n",
    "\n",
    "model = SimpleCNN()  # Instantiate the model.\n",
    "criterion = nn.CrossEntropyLoss()  # Define the loss function.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Define the optimizer.\n",
    "\n",
    "# Training function.\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()  # Set the model to training mode.\n",
    "    for epoch in range(num_epochs):  # Loop over the number of epochs.\n",
    "        running_loss = 0.0  # Initialize the running loss.\n",
    "        for images, labels in dataloader:  # Loop over the data in the dataloader.\n",
    "            optimizer.zero_grad()  # Zero the gradients.\n",
    "            outputs = model(images)  # Forward pass.\n",
    "            loss = criterion(outputs, labels)  # Compute the loss.\n",
    "            loss.backward()  # Backward pass.\n",
    "            optimizer.step()  # Update the parameters.\n",
    "            running_loss += loss.item()  # Accumulate the loss.\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}')  # Print the average loss for the epoch.\n",
    "\n",
    "# Uncertainty sampling function.\n",
    "def get_uncertain_samples(model, dataloader, k):\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    uncertainties = []  # Initialize a list to store uncertainties.\n",
    "    with torch.no_grad():  # Disable gradient calculation.\n",
    "        for images, _ in dataloader:  # Loop over the data in the dataloader.\n",
    "            outputs = model(images)  # Forward pass.\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Apply softmax to get probabilities.\n",
    "            entropy = -torch.sum(probabilities * torch.log(probabilities + 1e-10), dim=1)  # Compute the entropy for each sample.\n",
    "            uncertainties.append(entropy)  # Append the entropy to the list.\n",
    "    uncertainties = torch.cat(uncertainties)  # Concatenate the list of uncertainties.\n",
    "    _, indices = torch.topk(uncertainties, k)  # Get the indices of the top k uncertainties.\n",
    "    return indices  # Return the indices.\n",
    "\n",
    "# Active Learning Loop.\n",
    "num_initial_samples = 5  # Number of samples to start training with.\n",
    "num_queries = 2  # Number of samples to query in each iteration.\n",
    "num_iterations = 1000  # Number of active learning iterations.\n",
    "\n",
    "# Initial training set.\n",
    "initial_indices = np.random.choice(len(X_train), num_initial_samples, replace=False)  # Randomly select initial samples.\n",
    "train_dataset = Subset(train_dataset, initial_indices)  # Create a subset of the training dataset with the initial samples.\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create a data loader for the initial training dataset.\n",
    "\n",
    "for iteration in range(num_iterations):  # Loop over the number of iterations.\n",
    "    print(f'Iteration {iteration+1}/{num_iterations}')  # Print the current iteration.\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs=1)  # Train the model for one epoch.\n",
    "\n",
    "    # Get the most uncertain samples.\n",
    "    remaining_indices = list(set(range(len(X_train))) - set(initial_indices))  # Get the remaining samples that are not in the training set.\n",
    "    if len(remaining_indices) == 0:  # If there are no remaining samples.\n",
    "        break  # Exit the loop.\n",
    "    remaining_dataset = Subset(AnimalDataset(X_train, y_train, transform=transform), remaining_indices)  # Create a subset of the remaining samples.\n",
    "    remaining_loader = DataLoader(remaining_dataset, batch_size=16, shuffle=False)  # Create a data loader for the remaining samples.\n",
    "    uncertain_indices = get_uncertain_samples(model, remaining_loader, min(num_queries, len(remaining_indices)))  # Get the indices of the most uncertain samples.\n",
    "\n",
    "    # Add the uncertain samples to the training set.\n",
    "    initial_indices = np.concatenate([initial_indices, uncertain_indices])  # Add the uncertain samples to the initial indices.\n",
    "    train_dataset = Subset(AnimalDataset(X_train, y_train, transform=transform), initial_indices)  # Create a new subset of the training dataset.\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Create a new data loader for the updated training dataset.\n",
    "\n",
    "# Final model evaluation.\n",
    "model.eval()  # Set the model to evaluation mode.\n",
    "correct = 0  # Initialize the number of correct predictions.\n",
    "total = 0  # Initialize the total number of samples.\n",
    "with torch.no_grad():  # Disable gradient calculation.\n",
    "    for images, labels in test_loader:  # Loop over the data in the test loader.\n",
    "        outputs = model(images)  # Forward pass.\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the predicted class.\n",
    "        total += labels.size(0)  # Update the total number of samples.\n",
    "        correct += (predicted == labels).sum().item()  # Update the number of correct predictions.\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')  # Print the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLA-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
